{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:06.854398",
     "start_time": "2016-10-18T14:31:34.464786"
    },
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Cleans up the Stock information\n",
    "nyse=pd.read_csv(\"Data/NYSE.csv\")\n",
    "nasdaq=pd.read_csv(\"Data/NASDAQ.csv\")\n",
    "amex=pd.read_csv(\"Data/AMEX.csv\")\n",
    "stocks=nyse.append(nasdaq,ignore_index=True).append(amex,ignore_index=True)\n",
    "del stocks[\"Summary Quote\"]\n",
    "i=0\n",
    "while i<len(stocks[\"Symbol\"]):\n",
    "    if \" (The)\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" (The)\")]\n",
    "    if \" Incorporated\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Incorporated\")]\n",
    "    if \", Inc.\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\", Inc.\")]\n",
    "    if \", Inc\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\", Inc\")]\n",
    "    if \", INC\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\", Inc\")]\n",
    "    if \" Inc.\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Inc.\")]\n",
    "    if \" Inc\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Inc\")]\n",
    "    if \" INC\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Inc\")]\n",
    "    if  \" Corporation\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Corporation\")]\n",
    "    if \" Corp.\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Corp.\")]\n",
    "    if \" Corp\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Corp\")]\n",
    "    if \" CORP\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Corp\")]\n",
    "    i+=1\n",
    "\n",
    "#Eliminates Duplicates, inactive tickers, and funds\n",
    "#For MarketCap n/a; the tickers listed had 1 market value and all other duplicates were n/a; safe to remove\n",
    "#Additionally, there was the occasional case of no market cap; market cap must have been insignificant or bankrupt\n",
    "#stocks= stocks[stocks['MarketCap']!=\"n/a\"]\n",
    "#stocks= stocks[stocks['industry']!=\"n/a\"]\n",
    "#stocks= stocks[stocks['Sector']!=\"n/a\"]\n",
    "#stocks= stocks[stocks['IPOyear']!=\"n/a\"]\n",
    "\n",
    "#Convert MarketCap to Millions of dollars\n",
    "i=0\n",
    "while i<len(stocks[\"MarketCap\"]):\n",
    "    if \"B\" in str(stocks.iloc[i,3])[-1:]:\n",
    "        stocks.iloc[i,3]=float(str(stocks.iloc[i,3])[1:-1])*1000\n",
    "    else:\n",
    "        stocks.iloc[i,3]=str(stocks.iloc[i,3])[1:-1]\n",
    "    i+=1\n",
    "\n",
    "stocks=stocks.reset_index()\n",
    "del stocks['index']\n",
    "#delete={1572:'WYIGU',1001:'ANDAU',873:'FNFV',1322:'ELECU',1376:'FCFS',1134:'CLACU',2041:'TRTLU',1706:'MSG'}\n",
    "# keys=delete.keys()\n",
    "# keys.sort()\n",
    "# keys=keys[::-1]\n",
    "# for key in keys:\n",
    "#     stocks=stocks.iloc[:key].append(stocks.iloc[key+1:])\n",
    "# stocks=stocks.reset_index()\n",
    "# del stocks['index']\n",
    "\n",
    "#Cleans cities data (eliminates counties and combined cities/towns)\n",
    "cities=pd.read_csv(\"Data/Living_Index.csv\")\n",
    "cities=cities.rename(columns={\"Urban Area\":\"City\"})\n",
    "values=[]\n",
    "for x in cities[\"City\"]:\n",
    "    if \"-\" in x:\n",
    "        values.append(False)\n",
    "    else:\n",
    "        values.append(True)\n",
    "cities=cities[values]\n",
    "values=[]\n",
    "for x in cities[\"City\"]:\n",
    "    if \"county\" in x.lower():\n",
    "        values.append(False)\n",
    "    else:\n",
    "        values.append(True)\n",
    "cities=cities[values]\n",
    "new_city_list=[]\n",
    "\n",
    "for x in cities[\"City\"]:\n",
    "    y=x[-4:]\n",
    "    z=x[:-len(y)].replace(\",\",\"\")+y\n",
    "    new_city_list.append(z)\n",
    "cities[\"City\"]=new_city_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:20.251782",
     "start_time": "2016-10-18T14:32:06.856716"
    },
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "#Put ticker of name instances into a dictionary for all duplicates\n",
    "\n",
    "#Gives the duplicates in a list called names\n",
    "names=[x for x,y in zip(stocks[\"Name\"].value_counts().sort_values(ascending=False).index,\\\n",
    "                        stocks[\"Name\"].value_counts().sort_values(ascending=False).values) if y>1]\n",
    "indices=[]\n",
    "index=[]\n",
    "dic={}\n",
    "i=0\n",
    "while i < len(stocks[\"Name\"]):\n",
    "    if stocks.loc[i,\"Name\"] in names:\n",
    "        if stocks.loc[i,\"Name\"] in dic.keys():\n",
    "            dic[stocks.loc[i,\"Name\"]].append([stocks.loc[i,\"Symbol\"],stocks.loc[i,\"MarketCap\"]])\n",
    "        else:\n",
    "            dic[stocks.loc[i,\"Name\"]]=[[stocks.loc[i,\"Symbol\"],stocks.loc[i,\"MarketCap\"]]]\n",
    "    i+=1\n",
    "\n",
    "#Look up to see if there is a single entry with a market cap for the duplicates; if there is, delete others\n",
    "\n",
    "#Checks to see if among duplicate entries there is only a single one with market cap\n",
    "caps=[]\n",
    "for key in dic.keys():\n",
    "    l=dic[key]\n",
    "    single_cap=False\n",
    "    num_caps=0\n",
    "    for x in l:\n",
    "        if x[1]!=\"/\":\n",
    "            num_caps+=1\n",
    "        if num_caps==1:\n",
    "            single_cap=True\n",
    "        else: \n",
    "            single_cap=False\n",
    "    caps.append([key,single_cap])\n",
    "\n",
    "#Finds tickers without market cap to delete\n",
    "delete=[]\n",
    "for x in caps:\n",
    "    if x[1]==True:\n",
    "            #If there is a single market cap value, deletes all others\n",
    "        for y in dic[x[0]]:\n",
    "            if y[1]==\"/\":\n",
    "                delete.append(y[0])\n",
    "    if x[1]==False:\n",
    "            #Checks to see if there are duplicate market cap values; if there are, resorts to largest\n",
    "        mult=False\n",
    "        count=0\n",
    "        for y in dic[x[0]]:\n",
    "            if y[1]!=\"/\":\n",
    "                count+=1\n",
    "            if count>1:\n",
    "                mult=True\n",
    "        largest=0\n",
    "        save_ticker=\"\"\n",
    "        if mult:\n",
    "            for y in dic[x[0]]:\n",
    "                if y[1]!=\"/\":\n",
    "                    if y[1]>largest:\n",
    "                        largest=y[1]\n",
    "                        save_ticker=y[0]\n",
    "            for z in dic[x[0]]:\n",
    "                if z[0]!=save_ticker:\n",
    "                    delete.append(z[0])\n",
    "            #If there are no market cap values, resorts to shortest ticker name\n",
    "        else:\n",
    "            save_ticker=\"\"\n",
    "            length=10\n",
    "            for y in dic[x[0]]:\n",
    "                if len(y[0])<length:\n",
    "                    save_ticker=y[1]\n",
    "            for z in dic[x[0]]:\n",
    "                if z[0]!=save_ticker:\n",
    "                    delete.append(z[0])\n",
    "\n",
    "#NOW, time to go through and delete the unneeded rows; \n",
    "    #reset the index, get the indices, reverse order the sort,\n",
    "    #and reset the dataframe on each value; then reset index\n",
    "\n",
    "indices=[]\n",
    "i=0\n",
    "while i<len(stocks):\n",
    "    if stocks.loc[i,\"Symbol\"] in delete:\n",
    "        indices.append(i)\n",
    "    i+=1\n",
    "indices.sort()\n",
    "indices=indices[::-1]\n",
    "for key in indices:\n",
    "    stocks=stocks.iloc[:key].append(stocks.iloc[key+1:])\n",
    "stocks=stocks.reset_index()\n",
    "del stocks['index']\n",
    "\n",
    "#There is only one case left; where there are duplicate entries; let's delete those now\n",
    "names=[x for x,y in zip(stocks[\"Name\"].value_counts().sort_values(ascending=False).index,\\\n",
    "                        stocks[\"Name\"].value_counts().sort_values(ascending=False).values) if y>1]\n",
    "delete=[]\n",
    "\n",
    "for name in names:\n",
    "    first=True\n",
    "    i=0\n",
    "    while i<len(stocks[\"Name\"]):\n",
    "        if first:\n",
    "            first=False\n",
    "        if stocks.loc[i,\"Name\"]==name:\n",
    "            delete.append(i)\n",
    "        i+=1\n",
    "delete.sort()\n",
    "delete=delete[::-1]\n",
    "for key in delete:\n",
    "    stocks=stocks.iloc[:key].append(stocks.iloc[key+1:])\n",
    "stocks=stocks.reset_index()\n",
    "del stocks['index']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.291552",
     "start_time": "2016-10-18T14:32:20.254019"
    },
    "collapsed": false
   },
   "source": [
    "#Takes in json of webscraped information, puts it into a dataframe, converts Salary information to integers\n",
    "glass_door=pd.DataFrame(columns=[\"Salary\",\"Company\",\"Location\"])\n",
    "\n",
    "extension=\"Data/allcities.json\"\n",
    "json=pd.read_json(extension)\n",
    "df=pd.DataFrame([json[\"city\"],json[\"company\"],json[\"meanPay\"],json[\"jobTitle\"]]).T\n",
    "df.columns=[\"Location\",\"Company\",\"Salary\",\"Job\"]\n",
    "temp=[]\n",
    "\n",
    "\n",
    "df['Salary'] = df['Salary'].apply(only_numerics)\n",
    "df=df[pd.notnull(df[\"Salary\"])].reset_index()\n",
    "del df['index']\n",
    "df[\"Salary\"]=df['Salary'].astype(np.float)#\n",
    "\n",
    "glass_door=glass_door.append(df,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "df.columns=[\"Location\",\"Company\",\"Salary\",\"Job\"]\n",
    "temp=[]\n",
    "\n",
    "\n",
    "df['Salary'] = df['Salary'].apply(only_numerics)\n",
    "df=df[pd.notnull(df[\"Salary\"])].reset_index()\n",
    "del df['index']\n",
    "df[\"Salary\"]=df['Salary'].astype(np.float)#\n",
    "\n",
    "glass_door=glass_door.append(df,ignore_index=True)\n",
    "\n",
    "df=pd.merge(glass_door,cities,left_on=\"Location\",right_on=\"City\")\n",
    "df=pd.merge(df,stocks,left_on=\"Company\",right_on=\"Name\",how='inner')\n",
    "i=0\n",
    "delete=[]\n",
    "for x in df[\"Salary\"]:\n",
    "    if x<10000:\n",
    "        delete.append(i)\n",
    "    i+=1\n",
    "delete.sort()\n",
    "delete=delete[::-1]\n",
    "for key in delete:\n",
    "    df=df.iloc[:key].append(df.iloc[key+1:])\n",
    "df=df.reset_index()\n",
    "del df['index']\n",
    "\n",
    "df['Salary']=df['Salary'].astype(np.int)\n",
    "df.columns\n",
    "\n",
    "\n",
    "# todrop=[\"Software Engineer (Data Scientist)\",\"Data Visualization Scientist\",\\\n",
    "#         \"Scientist, Statistical and Data Sciences\",\"Associate Data Scientist\",\"Data Scientist Intern - Hourly\",\\\n",
    "#         \"Data Scientist Intern\"]\n",
    "\n",
    "\n",
    "df[\"Senior\"]=[1 if x else 0 for x in df[\"Job\"]==(\"Senior Data Scientist\")]\n",
    "df[\"Mid\"]=[1 if x else 0 for x in df[\"Job\"]==(\"Data Scientist\" or \"Principal Data Scientist\")]\n",
    "#df[\"Junior\"]=[1 if x else 0 for x in df[\"Job\"]==(\"Data Scientist I\" or \"Junior Data Scientist\" \n",
    "#                                                 or \"Data Scientist II\")]\n",
    "df[\"Large\"]=[1 if x else 0 for x in df[\"MarketCap\"]>=500]\n",
    "df[\"Small\"]=[1 if x else 0 for x in df[\"MarketCap\"]<100]\n",
    "i=0\n",
    "while i<len(df):\n",
    "    if df.loc[i,\"MarketCap\"] is np.NAN:\n",
    "        df.loc[i,\"MarketCap\"]=350\n",
    "    i+=1\n",
    "df[\"MarketCap\"]=df[\"MarketCap\"].astype(np.float)\n",
    "\n",
    "copy=df.copy()\n",
    "print len(df[df[\"Senior\"]==1])\n",
    "print len(df[df[\"Mid\"]==1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.292507",
     "start_time": "2016-10-18T18:31:34.482Z"
    },
    "collapsed": false
   },
   "source": [
    "240/92."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.293044",
     "start_time": "2016-10-18T18:31:34.486Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "plt.hist(df[df[\"Senior\"]==1][\"Salary\"],bins=10);\n",
    "plt.hist((df[df[\"Mid\"]==1][\"Salary\"]),bins=26);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.293572",
     "start_time": "2016-10-18T18:31:34.490Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "n=3\n",
    "increment=(199000-48000.)/float(n)\n",
    "bins=[48000,85750,148667,199000]\n",
    "bins=[]\n",
    "for i in range(n+1):\n",
    "    bins.append(48000+increment*(i))\n",
    "\n",
    "replacements=[]\n",
    "i=0\n",
    "while i<len(bins)-1:\n",
    "    replacements.append(str(int(round(bins[i])))+\"-\"+str(int(round(bins[i+1]))))\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "df=copy.copy()\n",
    "df[\"Salary\"]=df['Salary'].apply(lambda x: salary_bin(x))\n",
    "df1=pd.get_dummies(df,columns=[\"Salary\"])\n",
    "\n",
    "cols=['100% Composite Index','Senior',\"Mid\",\"Large\"]\n",
    "# for y in [x for x in df1.columns[20:]]:\n",
    "#     cols.append(y) \n",
    "    \n",
    "    \n",
    "X_train,X_test,y_train,y_test=train_test_split(df1[cols],df['Salary'],test_size=.4)#,random_state=69\n",
    "\"\"\" Fit a binary classification predictor.\"\"\"\n",
    "logreg = LogisticRegression(solver='lbfgs') #'newton-cg', 'lbfgs', 'liblinear', 'sag'\n",
    "C_vals = [10**x for x in range (-5,5)]\n",
    "penalties = ['l2']\n",
    "\n",
    "gs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "logreg = LogisticRegression(C=gs.best_params_['C'], penalty=gs.best_params_['penalty'])\n",
    "cv_model = logreg.fit(X_train, y_train)\n",
    "cv_pred = cv_model.predict(X_test)\n",
    "\n",
    "y_score = cv_model.decision_function(X_test) # Submit these response, the output of model.decision_function\n",
    "\n",
    "\"\"\" Check your performance so far.\"\"\"\n",
    "\n",
    "conmat = np.array(confusion_matrix(y_test, cv_pred))\n",
    "confusion = pd.DataFrame(conmat, index=replacements, columns=replacements)\n",
    "\n",
    "print(confusion)\n",
    "print classification_report(y_test,cv_pred)\n",
    "print cv_model.coef_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.294048",
     "start_time": "2016-10-18T18:31:34.494Z"
    },
    "collapsed": false
   },
   "source": [
    "\"\"\" Fit a binary classification predictor.\"\"\"\n",
    "logreg = LogisticRegression(solver='liblinear') #'newton-cg', 'lbfgs', 'liblinear', 'sag'\n",
    "C_vals = [10**x for x in range (-5,5)]\n",
    "penalties = ['l1','l2']\n",
    "\n",
    "gs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "logreg = LogisticRegression(C=gs.best_params_['C'], penalty=gs.best_params_['penalty'])\n",
    "cv_model = logreg.fit(X_train, y_train)\n",
    "cv_pred = cv_model.predict(X_test)\n",
    "\n",
    "y_score = cv_model.decision_function(X_test) # Submit these response, the output of model.decision_function\n",
    "\n",
    "\"\"\" Check your performance so far.\"\"\"\n",
    "\n",
    "conmat = np.array(confusion_matrix(y_test, cv_pred))\n",
    "confusion = pd.DataFrame(conmat, index=replacements,\n",
    "                            columns=replacements)\n",
    "\n",
    "print(confusion)\n",
    "print classification_report(y_test,cv_pred)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.294526",
     "start_time": "2016-10-18T18:31:34.500Z"
    },
    "collapsed": false
   },
   "source": [
    "roc_auc_score(y_test, y_score)\n",
    "\n",
    "\"\"\" Plot AUC\"\"\"\n",
    "\n",
    "FPR = dict()\n",
    "TPR = dict()\n",
    "ROC_AUC = dict()\n",
    "\n",
    "# For class 1, find the area under the curve\n",
    "FPR[1], TPR[1], _ = roc_curve(y_test, y_score)\n",
    "ROC_AUC[1] = auc(FPR[1], TPR[1])\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for high/low income', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.295061",
     "start_time": "2016-10-18T18:31:34.503Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "#Linear Regression\n",
    "xtr=x_train\n",
    "xte=x_test\n",
    "ytr=y_train\n",
    "yte=y_test\n",
    "\n",
    "xtr=np.array(xtr).reshape(len(xtr),4)\n",
    "xte=np.array(xte).reshape(len(xte),4)\n",
    "xtr2=sm.add_constant(xtr)\n",
    "model=sm.OLS(ytr,xtr).fit()\n",
    "\n",
    "xte2=sm.add_constant(xte)\n",
    "pred=model.predict(xte)\n",
    "\n",
    "plt.plot(pred,yte,'o')\n",
    "plt.plot(pred,pred,'-')\n",
    "plt.xlim(0,180000)\n",
    "plt.ylim(0,180000)\n",
    "plt.show()\n",
    "\n",
    "print model.summary2()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.295965",
     "start_time": "2016-10-18T18:31:34.507Z"
    },
    "collapsed": false
   },
   "source": [
    "temp=xtr[:len(xtr)/3]\n",
    "plt.hist(xte[\"100% Composite Index\"],bins=5)\n",
    "plt.hist(temp[\"100% Composite Index\"],bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.296807",
     "start_time": "2016-10-18T18:31:34.510Z"
    },
    "collapsed": false
   },
   "source": [
    "df.iloc[730]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For the logistic regressions, make the discrete value a bin; the smallest bin will be 0. The goal should be to make the bins be organized so that a linear plot can be drawn through them. If the linear plot does not run through them, then there is significant risk of misidentifying a bin. Depending on what the x-axis is (most-likely score), the bins will have to have an equal width along this x-axis (i.e., all bins having a width of x-units on the x-axis).\n",
    "\n",
    "Contrarily, we want evenly distributed bins in the price aspect.\n",
    "\n",
    "So, if one were to not do a multinomial logistic regression, one could additionally do a series of logistic regressions. One would check to see if the point goes into bin 0 or 1. If it goes into 1, one would check to see if it goes into bins 1 or 2. If it is in bin 2, one would see if it goes into bins 2 or 3... If it does not go into the higher bin, the regression sequence ends, and the point enters the lower of the two bins.\n",
    "### Scratch ^; essentially a multinomial logistic regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.297363",
     "start_time": "2016-10-18T18:31:34.520Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.297797",
     "start_time": "2016-10-18T18:31:34.525Z"
    },
    "collapsed": false
   },
   "source": [
    "#Merge Company Data and Location Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.298256",
     "start_time": "2016-10-18T18:31:34.530Z"
    },
    "collapsed": false
   },
   "source": [
    "plt.hist(glass_door[glass_door[\"Job\"]==\"Senior Data Scientist\"][\"Salary\"],bins=10)\n",
    "plt.hist(glass_door[glass_door[\"Job\"]==\"Data Scientist\"][\"Salary\"],bins=36);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.298696",
     "start_time": "2016-10-18T18:31:34.534Z"
    },
    "collapsed": false
   },
   "source": [
    "glass_door=glass_door.merge(cities, left_on=\"Location\", right_on=\"City\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.299150",
     "start_time": "2016-10-18T18:31:34.536Z"
    },
    "collapsed": false
   },
   "source": [
    "glass_door=glass_door.merge(cities, left_on=\"Location\", right_on=\"City\")\n",
    "glass_door[\"Normalized Salary\"]=glass_door[\"Salary\"]/glass_door[\"100% Composite Index\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.299584",
     "start_time": "2016-10-18T18:31:34.568Z"
    },
    "collapsed": false
   },
   "source": [
    "glass_door"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:32:22.300062",
     "start_time": "2016-10-18T18:31:34.573Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "df=glass_door.copy()\n",
    "df[\"Senior\"]=[1 if x else 0 for x in df[\"Job\"]==(\"Senior Data Scientist\" or \"Principal Data Scientist\")]\n",
    "df[\"Mid\"]=[1 if x else 0 for x in df[\"Job\"]==(\"Data Scientist\")]\n",
    "    \n",
    "#df=df[df[\"Job\"]==(\"Data Scientist\" or \"Principal Data Scientist\" or \"Senior Data Scientist\")]\n",
    "cols=['Senior',\"Mid\"]\n",
    "    \n",
    "    \n",
    "x_train,x_test,y_train,y_test=train_test_split(df[cols],df['Normalized Salary'],test_size=.35,random_state=70)\n",
    "\"\"\" Fit a binary classification predictor.\"\"\"\n",
    "#Linear Regression\n",
    "xtr=x_train\n",
    "xte=x_test\n",
    "ytr=y_train\n",
    "yte=y_test\n",
    "\n",
    "xtr=np.array(xtr).reshape(len(xtr),2)\n",
    "xte=np.array(xte).reshape(len(xte),2)\n",
    "xtr2=sm.add_constant(xtr)\n",
    "model=sm.OLS(ytr,xtr).fit()\n",
    "\n",
    "xte2=sm.add_constant(xte)\n",
    "pred=model.predict(xte)\n",
    "\n",
    "plt.plot(pred,yte,'o')\n",
    "plt.plot(pred,pred,'-')\n",
    "plt.xlim(0,2000)\n",
    "plt.ylim(0,2000)\n",
    "plt.show()\n",
    "\n",
    "print model.summary2()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.691773",
     "start_time": "2016-10-18T14:52:34.646915"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import urllib\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "% matplotlib inline\n",
    "lm = linear_model.LinearRegression()\n",
    "LR=sklearn.linear_model.LogisticRegression\n",
    "LRCV=sklearn.linear_model.LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.814678",
     "start_time": "2016-10-18T14:52:34.695402"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def renaming(x):\n",
    "    new_name=\"\"\n",
    "    if x==(\"Junior Data Scientist\"or\"Entry Level Data Scientist\" or \"Data Scientist II\" or \"Associate Data Scientist\"):\n",
    "        new_name=\"Junior DS\"\n",
    "    elif x==(\"Senior Data Scientist\" or \"Principal Data Scientist\"):\n",
    "        new_name==\"Senior DS\"\n",
    "    else:\n",
    "        new_name=\"DS\"\n",
    "    return new_name\n",
    "def name_change(x):\n",
    "    new_value=\"\"\n",
    "    if x in change.keys():\n",
    "        new_value=change[x]\n",
    "    else:\n",
    "        new_value=x\n",
    "    return new_value\n",
    "def cap(x):\n",
    "    value=0\n",
    "    if x is np.NAN:\n",
    "        value=\"Unknown\"\n",
    "    elif x>2000:\n",
    "        value=\"Hugecap\"\n",
    "    elif x>500:\n",
    "        value=\"Largecap\"\n",
    "    elif x>100:\n",
    "        value=\"Midcap\"\n",
    "    else:\n",
    "        value=\"Smallcap\"\n",
    "    return value\n",
    "def create_bins(low,high,n):\n",
    "    increment=(high-low)/float(n)\n",
    "    bins=[]\n",
    "    for i in range(n+1):\n",
    "        bins.append(low+increment*(i))\n",
    "    replacements=[]\n",
    "    i=0\n",
    "    while i<len(bins)-1:\n",
    "        replacements.append(str(int(round(bins[i])))+\"-\"+str(int(round(bins[i+1]))))\n",
    "        i+=1\n",
    "    return bins,replacements\n",
    "def salary_bin(bins,replacements,x):\n",
    "    i=0\n",
    "    value=0\n",
    "    while i<len(bins)-1:\n",
    "        if x<=int(round(bins[i+1])):\n",
    "            value=replacements[i]\n",
    "            i=len(bins)\n",
    "        i+=1\n",
    "    if value==0:\n",
    "        value=str(bins[-1])+\"+\"\n",
    "    return value\n",
    "def only_numerics(seq):\n",
    "    seq = seq.split('-')\n",
    "    try:\n",
    "        out = ((int(filter(type(seq[0]).isdigit, seq[0])) + int(filter(type(seq[1]).isdigit, seq[1])))/2)*1000\n",
    "    except:\n",
    "        out = int(filter(type(seq[0]).isdigit, seq[0]))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Uploads the non-Glass Door data.\n",
    "The stock data is cleaned for:\n",
    "* Certain terms in the name. Things like 'inc,' or 'corp' are deleted. This helps the matching process.\n",
    "* The market cap information is converted from strings to millions of dollars.\n",
    "\n",
    "The city data is cleaned for:\n",
    "\n",
    "* Cities containing '-'\n",
    "* Cities containing 'county'\n",
    "* Removing ',' from city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:53:21.979908",
     "start_time": "2016-10-18T14:53:21.834621"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleans up the Stock information\n",
    "nyse=pd.read_csv(\"Data/NYSE.csv\")\n",
    "nasdaq=pd.read_csv(\"Data/NASDAQ.csv\")\n",
    "amex=pd.read_csv(\"Data/AMEX.csv\")\n",
    "stocks=nyse.append(nasdaq,ignore_index=True).append(amex,ignore_index=True)\n",
    "del stocks[\"Summary Quote\"]\n",
    "i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:37:15.643836",
     "start_time": "2016-10-18T14:37:15.635873"
    },
    "collapsed": true
   },
   "source": [
    "We had discrepancies between the names of the scraped Glass Door data, and the names of the stock information. This led to information not being merged when it should have. So, we manually searched for the pairs of names and input them into a dictionary. This dictionary would be used to rename the mislabeled Glass Door entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:53:24.748348",
     "start_time": "2016-10-18T14:53:22.664434"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"ONLY USED FOR FINDING STOCKS\"\"\"\n",
    "\n",
    "names=[x for x,y in zip(stocks[\"Name\"].value_counts().sort_values(ascending=False).index,\\\n",
    "                        stocks[\"Name\"].value_counts().sort_values(ascending=False).values) if y>1]\n",
    "\n",
    "names=['Price']\n",
    "\n",
    "#Finds all instances of a company name within stocks[\"Name\"], and prints the instance and its index numer\n",
    "for name in names:\n",
    "    i=0\n",
    "    while i<len(stocks):\n",
    "        if stocks.loc[i,\"Name\"]==name:\n",
    "            print \"index: \",i\n",
    "            print stocks.iloc[i]\n",
    "            print \"\\n\"\n",
    "        i+=1\n",
    "\n",
    "# [x for x in stocks[\"Name\"] if x[0:3]==\"Exp\"] #-->Additional Search Method; looks for first three letters\n",
    "# [x for x in stocks[\"Name\"] if \"Price\".lower() in x.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.883609",
     "start_time": "2016-10-18T18:52:34.642Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "change={\"GE\": \"General Electric Company\", \"IBM\":\"Microsoft\",\"Capital One\":\"Capital One Financial\",\n",
    "        \"Verizon\":\"Verizon Communications\",\"Hewlett Packard Enterprise | HPE\":'Hewlett Packard Enterprise Company',\n",
    "       \"Hewlett-Packard\":'Hewlett Packard Enterprise Company',\"Alliance Data\":\"Alliance Data Systems\",\n",
    "       'Raytheon':'Raytheon Company',\"GE\":'General Electric Company',\"Verizon Wireless\":\"Verizon Communications\",\n",
    "       \"Intel Corporation\":\"Intel\",\"MaxPoint\":\"MaxPoint Interactive\",\"General Motors\":\"General Motors Company\",\n",
    "       \"Booz Allen Hamilton\":\"Booz Allen Hamilton Holding\",\"Nielsen\":\"Nielsen N.V.\",\"SAP\":\"SAP SE\",\n",
    "       \"General Dynamics Mission Systems\":\"General Dynamics\",\"Leidos\":\"Leidos Holdings\",\n",
    "       \"Honeywell\":\"Honeywell International\",\"Akamai\":\"Akamai Technologies\",\"BNY Mellon\":\"Bank Of New York Mellon\",\n",
    "       \"FIS\":\"Fidelity National Information Services\",\"J.P. Morgan\":'J P Morgan Chase & Co',\n",
    "       \"Walmart\":'Wal-Mart Stores',\"PayPal\":\"PayPal Holdings\",\"Adobe\":\"Adobe Systems\",'AOL':\"Verizon Communications\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.884137",
     "start_time": "2016-10-18T18:52:34.646Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while i<len(stocks[\"Symbol\"]):\n",
    "    if \" (The)\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" (The)\")]\n",
    "    if \" Incorporated\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Incorporated\")]\n",
    "    if \", Inc.\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\", Inc.\")]\n",
    "    if \", Inc\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\", Inc\")]\n",
    "    if \", INC\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\", Inc\")]\n",
    "    if \" Inc.\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Inc.\")]\n",
    "    if \" Inc\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Inc\")]\n",
    "    if \" INC\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Inc\")]\n",
    "    if  \" Corporation\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Corporation\")]\n",
    "    if \" Corp.\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Corp.\")]\n",
    "    if \" Corp\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Corp\")]\n",
    "    if \" CORP\" in stocks.iloc[i,1]:\n",
    "        stocks.iloc[i,1]=stocks.iloc[i,1][:len(stocks.iloc[i,1])-len(\" Corp\")]\n",
    "    i+=1\n",
    "\n",
    "#Convert MarketCap to Millions of dollars\n",
    "i=0\n",
    "while i<len(stocks[\"MarketCap\"]):\n",
    "    if \"B\" in str(stocks.iloc[i,3])[-1:]:\n",
    "        stocks.iloc[i,3]=float(str(stocks.iloc[i,3])[1:-1])*1000\n",
    "    else:\n",
    "        stocks.iloc[i,3]=str(stocks.iloc[i,3])[1:-1]\n",
    "    i+=1\n",
    "\n",
    "stocks=stocks.reset_index()\n",
    "del stocks['index']\n",
    "\n",
    "#Cleans cities data (eliminates counties and combined cities/towns)\n",
    "cities=pd.read_csv(\"Data/Living_Index.csv\")\n",
    "cities=cities.rename(columns={\"Urban Area\":\"City\"})\n",
    "values=[]\n",
    "for x in cities[\"City\"]:\n",
    "    if \"-\" in x:\n",
    "        values.append(False)\n",
    "    else:\n",
    "        values.append(True)\n",
    "cities=cities[values]\n",
    "values=[]\n",
    "for x in cities[\"City\"]:\n",
    "    if \"county\" in x.lower():\n",
    "        values.append(False)\n",
    "    else:\n",
    "        values.append(True)\n",
    "cities=cities[values]\n",
    "new_city_list=[]\n",
    "\n",
    "for x in cities[\"City\"]:\n",
    "    y=x[-4:]\n",
    "    z=x[:-len(y)].replace(\",\",\"\")+y\n",
    "    new_city_list.append(z)\n",
    "cities[\"City\"]=new_city_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate a Glass Door data frame to connect the json to.\n",
    "We upload the json, which has four columns:\n",
    "* Location\n",
    "* Company\n",
    "* Salary\n",
    "* Job\n",
    "\n",
    "We clean the json df for:\n",
    "* Changing salary ranges to averagees\n",
    "\n",
    "We save the json df as 'glass_door'.\n",
    "\n",
    "We clean the glass_door df for:\n",
    "* Name changes found listed above\n",
    "\n",
    "We merge the glass_door df with cities. \n",
    "\n",
    "We create a new column \"Normalized Salary\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.884614",
     "start_time": "2016-10-18T18:52:34.651Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glass_door=pd.DataFrame(columns=[\"Salary\",\"Company\",\"Location\"])\n",
    "\n",
    "extension=\"Data/allcities.json\"\n",
    "json=pd.read_json(extension)\n",
    "df=pd.DataFrame([json[\"city\"],json[\"company\"],json[\"meanPay\"],json[\"jobTitle\"]]).T\n",
    "df.columns=[\"Location\",\"Company\",\"Salary\",\"Job\"]\n",
    "temp=[]\n",
    "\n",
    "\n",
    "df['Salary'] = df['Salary'].apply(only_numerics)\n",
    "df=df[pd.notnull(df[\"Salary\"])].reset_index()\n",
    "del df['index']\n",
    "df[\"Salary\"]=df['Salary'].astype(np.float)#\n",
    "\n",
    "glass_door=glass_door.append(df,ignore_index=True)\n",
    "\n",
    "glass_door[\"Company\"]=glass_door[\"Company\"].apply(lambda x: name_change(x))\n",
    "\n",
    "glass_door=glass_door.merge(cities, left_on=\"Location\", right_on=\"City\")\n",
    "glass_door[\"Normalized Salary\"]=glass_door[\"Salary\"]/glass_door[\"100% Composite Index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the glass_door df with stocks. We only select those Jobs that are Junior Level, Middle Level, or Senior Level. We dont want to end up with internships. We then bin the jobs with their respective bin names (as just mentioned).\n",
    "\n",
    "For the most recurring private companies, market cap and sectors were updated as they had an impact on prediction ability.\n",
    "\n",
    "Null values in market cap and sector were changed appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.885089",
     "start_time": "2016-10-18T18:52:34.655Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=glass_door.copy()\n",
    "df=df.merge(stocks, how='left',left_on=\"Company\",right_on=\"Name\")\n",
    "df=df[df[\"Job\"]==(\"Senior Data Scientist\" or \"Principal Data Scientist\" or \"Data Scientist\" or \"Junior Data Scientist\"\n",
    "                 or \"Entry Level Data Scientist\" or \"Data Scientist II\" or \"Associate Data Scientist\")]\n",
    "\n",
    "\n",
    "df[\"Job\"]=df[\"Job\"].apply(lambda x: renaming(x))\n",
    "\n",
    "\n",
    "\n",
    "marketcap_sectors={'KPMG':[24440,\"Consulting\"],'Razorfish':[np.NaN,\"Technology\"],\n",
    "                   'Elevate Credit':[640,\"Finance\"],'BCG Digital Ventures':[np.NaN,\"Finance\"]}\n",
    "\n",
    "df=df.reset_index()\n",
    "del df[\"index\"]\n",
    "i=0\n",
    "while i<len(df):\n",
    "    if df.loc[i,\"Company\"] in marketcap_sectors.keys():\n",
    "        df.loc[i,\"MarketCap\"]=marketcap_sectors[df.loc[i,\"Company\"]][0]\n",
    "        df.loc[i,\"Sector\"]=marketcap_sectors[df.loc[i,\"Company\"]][1]\n",
    "    i+=1\n",
    "\n",
    "df[\"Sector\"]=[\"Technology\" if pd.isnull(x) else x for x in df[\"Sector\"]]\n",
    "df[\"MarketCap\"]=df[\"MarketCap\"].apply(lambda x: cap(x))\n",
    "#df[\"MarketCapSector\"]=df[\"Sector\"]+df[\"MarketCap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.885564",
     "start_time": "2016-10-18T18:52:34.660Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2=pd.get_dummies(df,columns=[\"Sector\",\"MarketCap\",\"State\",\"Region\"])\n",
    "delete=[u'industry',u'Company', u'Location', u'Salary', u'City',\n",
    "       u'Symbol', u'Name', u'LastSale', u'IPOyear',u'Normalized Salary',u'100% Composite Index',\"Job\"] \n",
    "#u'13 % Grocery Items', u'29 % Housing',\n",
    "#      u'10% Utilities', u'12 % Transportation', u'4% Health Care',\n",
    "#       u'32 % Miscellaneous Goods and Services',\n",
    "for x in delete:\n",
    "    del df2[x]\n",
    "\n",
    "normalizer=pd.DataFrame(df[\"100% Composite Index\"].copy(),columns=[\"100% Composite Index\"])\n",
    "df2=df2.reset_index()\n",
    "del df2['index']\n",
    "normalizer=normalizer.reset_index()\n",
    "del normalizer['index']\n",
    "i=0\n",
    "cols=df2.columns\n",
    "while i<len(df2):\n",
    "    for x in cols:\n",
    "        df2.loc[i,x]=df2.loc[i,x]*normalizer.loc[i,\"100% Composite Index\"]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.886027",
     "start_time": "2016-10-18T18:52:34.662Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ENTER BINS BELOW\n",
    "bin_NUMBER=3\n",
    "\n",
    "temp2=pd.DataFrame(df[\"Salary\"].copy(),columns=[\"Salary\"])\n",
    "bins2,replacements2=create_bins(min(temp2[\"Salary\"]),max(temp2[\"Salary\"]),bin_NUMBER)\n",
    "\n",
    "temp=pd.DataFrame(df[\"Salary\"].copy(),columns=[\"Salary\"])\n",
    "bins,replacements=create_bins(min(temp[\"Salary\"]),max(temp[\"Salary\"]),bin_NUMBER)\n",
    "temp[\"Normalized Salary\"]=temp[\"Salary\"].apply(lambda x: salary_bin(bins,replacements,x))\n",
    "#df1=pd.get_dummies(temp,columns=[\"Normalized Salary\"])\n",
    "df1=temp[\"Normalized Salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.886510",
     "start_time": "2016-10-18T18:52:34.666Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prec=[]\n",
    "recall=[]\n",
    "f1score=[]\n",
    "max_rand=20\n",
    "\n",
    "for random in range(1,max_rand):\n",
    "    x_train,x_test,y_train,y_test=train_test_split(df2,df1,test_size=.35,stratify=df1,random_state=random)\n",
    "    logreg = LogisticRegression(solver='lbfgs') #'newton-cg', 'lbfgs', 'liblinear', 'sag'\n",
    "    C_vals = np.linspace(1,1001,10)\n",
    "    penalties = ['l2']\n",
    "\n",
    "    gs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, cv=3)\n",
    "    gs.fit(x_train, y_train)\n",
    "\n",
    "    logreg = LogisticRegression(C=gs.best_params_['C'], penalty=gs.best_params_['penalty'])\n",
    "    cv_model = logreg.fit(x_train, y_train)\n",
    "    cv_pred = cv_model.predict(x_test)\n",
    "\n",
    "    y_score = cv_model.decision_function(x_test) # Submit these response, the output of model.decision_function\n",
    "\n",
    "    \"\"\" Check your performance so far.\"\"\"\n",
    "\n",
    "    conmat = np.array(confusion_matrix(y_test, cv_pred))\n",
    "    confusion = pd.DataFrame(conmat, index=cv_model.classes_, columns=cv_model.classes_)\n",
    "    \n",
    "    prec.append(float(classification_report(y_test,cv_pred)[-35:-31]))\n",
    "    recall.append(float(classification_report(y_test,cv_pred)[-25:-21]))\n",
    "    f1score.append(float(classification_report(y_test,cv_pred)[-15:-11]))\n",
    "#    \n",
    "print \"Number of bins:    \", bin_NUMBER\n",
    "print \"Random Iterations: \", max_rand-1\n",
    "print \n",
    "print \"Average Precision: \", np.mean(prec)\n",
    "print \"Average Recall:    \", np.mean(recall)\n",
    "print \"Average F1-Score:  \", np.mean(f1score)\n",
    "print \n",
    "print \"Variance Precision:\", np.var(prec)\n",
    "print \"Variance Recall:   \", np.var(recall)\n",
    "print \"Variance F1-Score: \", np.var(f1score)\n",
    "\n",
    "\n",
    "#     print(confusion)\n",
    "#     print classification_report(y_test,cv_pred)\n",
    "    #print cv_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST with Junior Jobs and no Job Dummies\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.756315789474\n",
    "* Average Recall:    0.754736842105\n",
    "* Average F1-Score:  0.751052631579\n",
    "\n",
    "## Best with No Junior Jobs and Job Dummies\n",
    "* Number of bins:    3\n",
    "* Average Precision:  0.755789473684\n",
    "* Average Recall:     0.754210526316\n",
    "* Average F1-Score:   0.75\n",
    "\n",
    "## Best with No Junior Jobs and no Job Dummies\n",
    "* Number of bins:    3\n",
    "* Average Precision:  0.756315789474\n",
    "* Average Recall:     0.754736842105\n",
    "* Average F1-Score:   0.751052631579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Company Changes:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.748947368421\n",
    "* Average Recall:    0.748947368421\n",
    "* Average F1-Score:  0.744736842105\n",
    "\n",
    "## With Private Company changes:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.755789473684\n",
    "* Average Recall:    0.754210526316\n",
    "* Average F1-Score:  0.75\n",
    "\n",
    "## With PC changes and no Job Dummies:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.756315789474\n",
    "* Average Recall:    0.754736842105\n",
    "* Average F1-Score:  0.751052631579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Company Changes:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.748947368421\n",
    "* Average Recall:    0.748947368421\n",
    "* Average F1-Score:  0.744736842105\n",
    "\n",
    "## With Private Company changes:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.755789473684\n",
    "* Average Recall:    0.754210526316\n",
    "* Average F1-Score:  0.75\n",
    "\n",
    "## With Private Company changes and Sector-MarketCap Bin:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.753157894737\n",
    "* Average Recall:    0.752105263158\n",
    "* Average F1-Score:  0.748421052632\n",
    "\n",
    "## With PC changes and no State Dummies:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.723684210526\n",
    "* Average Recall:    0.726842105263\n",
    "* Average F1-Score:  0.72\n",
    "\n",
    "## With PC changes and no Region Dummies:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.754210526316\n",
    "* Average Recall:    0.754210526316\n",
    "* Average F1-Score:  0.748421052632\n",
    "\n",
    "## With PC changes and no Job Dummies:\n",
    "* Number of bins:    3\n",
    "* Average Precision: 0.756315789474\n",
    "* Average Recall:    0.754736842105\n",
    "* Average F1-Score:  0.751052631579\n",
    "\n",
    "## With PC changes, no J D, and no MarketCap Dummies\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:49:35.406474",
     "start_time": "2016-10-18T14:49:35.380856"
    },
    "collapsed": false
   },
   "source": [
    "print classification_report(y_test,cv_pred)[-35:]\n",
    "prec=classification_report(y_test,cv_pred)[-35:-31]\n",
    "recall=classification_report(y_test,cv_pred)[-25:-21]\n",
    "f1score=classification_report(y_test,cv_pred)[-15:-11]\n",
    "\n",
    "print prec\n",
    "print recall\n",
    "print f1score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:49:36.144281",
     "start_time": "2016-10-18T14:49:36.121777"
    },
    "collapsed": false
   },
   "source": [
    "cv_model.classes_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:49:36.830309",
     "start_time": "2016-10-18T14:49:36.819957"
    },
    "collapsed": false
   },
   "source": [
    "#plt.hist(df[\"Salary\"],bins=30)\n",
    "172*len(df[df[\"Salary\"]>145000])/float(len(df[\"Salary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.886965",
     "start_time": "2016-10-18T18:52:34.745Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gs.best_params_['C'], gs.best_params_['penalty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.887410",
     "start_time": "2016-10-18T18:52:34.749Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train2,x_test2,y_train2,y_test2=train_test_split(df2,df['Salary'],test_size=.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.887921",
     "start_time": "2016-10-18T18:52:34.751Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Fit a binary classification predictor.\"\"\"\n",
    "#Linear Regression\n",
    "xtr=x_train2\n",
    "xte=x_test2\n",
    "ytr=y_train2\n",
    "yte=y_test2\n",
    "\n",
    "xtr=np.array(xtr).reshape(len(xtr),len(x_train2.columns))\n",
    "xte=np.array(xte).reshape(len(xte),len(x_train2.columns))\n",
    "xtr2=sm.add_constant(xtr)\n",
    "model=sm.OLS(ytr,xtr).fit()\n",
    "\n",
    "xte2=sm.add_constant(xte)\n",
    "pred=model.predict(xte)\n",
    "\n",
    "plt.plot(pred,yte,'o')\n",
    "plt.plot(pred,pred,'-')\n",
    "plt.xlim(0,200000)\n",
    "plt.ylim(0,200000)\n",
    "plt.show()\n",
    "\n",
    "print model.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.888402",
     "start_time": "2016-10-18T18:52:34.754Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ridge=Ridge(alpha=10) #Ridge is .28269 at 230\n",
    "lasso=Lasso(alpha=0.5) #Lasso is 0.289105 at 2.01\n",
    "lr=LinearRegression() #Linear Regression is .2763265\n",
    "model=lasso.fit(x_train2,y_train2)\n",
    "pred=model.predict(x_test2)\n",
    "plt.scatter(pred,y_test2)\n",
    "plt.plot(pred,pred)\n",
    "plt.ylim(0,200000)\n",
    "plt.xlim(0,200000)\n",
    "plt.show()\n",
    "print model.score(x_test2, y_test2)\n",
    "print \"\\n\"\n",
    "for x,y in zip(x_test2.columns,model.coef_):\n",
    "    print x\n",
    "    print y\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-18T14:52:34.888881",
     "start_time": "2016-10-18T18:52:34.756Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##MAKE THIS PRETTIER; We will present with this\n",
    "\n",
    "average=np.mean(glass_door[\"100% Composite Index\"])\n",
    "#\\plt.hist(glass_door[\"Salary\"]/average,width=50)\n",
    "plt.hist(glass_door[\"Salary\"],bins=30,width=4000)\n",
    "plt.hist(glass_door[\"Normalized Salary\"]*115,bins=30,width=4000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
