{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Intro to Dimensionality Reduction\n",
    "Week 7 | Lesson 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Follow the logical workflow behind dimensionality reduction\n",
    "- Describe the basic intuition of Principal Component Analysis\n",
    "- Calculate eigenvectors and eigenvalues for use in Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Have a working understand of scikit learn and numpy\n",
    "- Be able to create functions from scratch in python\n",
    "- Have a basic understanding of linear algebra concepts such as matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 10 min  | [Introduction](#introduction)   | Introduction to Dimensionality Reduction |\n",
    "| 15 min  | [Demo](#demo)  | Applications of Dimensionality Reduction: A Long-Form Approach  |\n",
    "| 25 min  | [Guided Practice](#guided-practice<a name=\"opening\"></a>)  | Conducting Dimensionality Analysis  |\n",
    "| 25 min  | [Independent Practice](#ind-practice)  | Dimensionality Reduction on the Iris Dataset  |\n",
    "| 5 min  | [Conclusion](#conclusion)  | Conclusion  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction: What is Dimensionality Reduction? (10 mins)\n",
    "\n",
    "Dimensionality reduction reduces the number of random variables that you are considering for analysis until you are left with the most important variables.\n",
    "\n",
    "Dimensionality reduction is not an end goal in itself, but a tool to form a dataset with more parsimonious features for further visualization and/or modelling.\n",
    "\n",
    "> Check: where have we already done dimensionality reduction? What are the potential benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine we have a linear graph, with one variable on the x axis and another on the y axis. Fitting a line models most of the information in the data (but leaves some noise). We can reduce the dimensions until the 45 degree line is completely horizontal - both of our measurements are now on the same plane - they are *one-dimensional*.\n",
    "\n",
    "![graph1](./assets/images/graph1.jpg)\n",
    "\n",
    "![graph2](./assets/images/graph2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Applications of Dimensionality Reduction (20 mins)\n",
    "\n",
    "Our first priority is to get comfortable with the initial manual workflow of PCA. (We'll expand on the math, applications and intuition in a following lesson.)\n",
    "\n",
    "- Isolate the feature data\n",
    "- Center and scale the feature data\n",
    "- Calculate their covariance matrix\n",
    "- Calculate the eigenvalues and eigenvectors\n",
    "- Choose the best n principal components\n",
    "- Calculate newly extracted feature data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "x = data.ix[selection].values\n",
    "y = data.ix[selection].values\n",
    "x_standard = StandardScaler().fit_transform(x)\n",
    "\n",
    "```\n",
    "\n",
    "A **covariance matrix** of n-features is just an n x n matrix, where the elements are the [covariances](https://en.wikipedia.org/wiki/Covariance) for each pair of _n_ features.\n",
    "\n",
    "```\n",
    "cov_mat = np.cov(x_standard.T)\n",
    "```\n",
    "\n",
    "(We're **transposing** the matrix only because np.cov expects features to be on the rows and columns to hold observations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we decompose our matrix by calling the numpy linear algebra function ```linalg.eig()```. to calculate the [**eigenvectors** and **eigenvalues**](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).\n",
    "\n",
    "```\n",
    "eigenValues, eigenVectors = np.linalg.eig(cov_mat)\n",
    "```\n",
    "\n",
    "The eigenvectors of a linear transformation are vectors that do not change direction under that transformation, but only have their magnitude scaled by some scalar value (the eigenvalue). In this context, the larger an eigenvalue, the more variance (information) in our data its corresponding eigenvector explains.\n",
    "\n",
    "Once we have our eigenvalues, we can work on transforming our data onto another dimensional space. Remember the visual representation from above - this is exactly what we are doing in this step. Don't worry about the mathematics of this for now, we'll touch on it later!\n",
    "\n",
    "Notice when calling ```linalg.eig``` from numpy, the input is limited to a matrix and the output requires two variables - the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Conducting Dimensionality Analysis (20 mins)\n",
    "\n",
    "Now that you know the procedure, let's run through an implementation of dimensionality reduction with a real dataset.\n",
    "\n",
    "We're going to be revisiting the [wine](./assets/datasets/wine_v.csv) dataset that lists the attributes of various different wine varieties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Varietal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>Cabernet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  Varietal  \n",
       "0      9.4        5  Cabernet  \n",
       "1      9.8        5  Cabernet  \n",
       "2      9.8        5  Cabernet  \n",
       "3      9.8        6  Cabernet  \n",
       "4      9.4        5  Cabernet  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('./assets/datasets/wine_v.csv')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Isolate the feature data.\"\"\"\n",
    "x = wine.ix[:,0:11].values\n",
    "y = wine.ix[:,12].values\n",
    "\n",
    "\"\"\" Center and scale the feature data.\"\"\"\n",
    "x_standard = StandardScaler().fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Calculate their covariance matrix. \"\"\"\n",
    "cov_mat = np.cov(x_standard.T)\n",
    "\n",
    "\"\"\"Calculate the eigenvalues and eigenvectors.\"\"\"\n",
    "eigenValues, eigenVectors = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.10107182,  1.92711489,  1.55151379,  1.21399175,  0.95989238,\n",
       "        0.05959558,  0.18144664,  0.34485779,  0.42322138,  0.58415655,\n",
       "        0.66002104])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48931422, -0.11050274, -0.12330157, -0.22961737, -0.08261366,\n",
       "        -0.63969145, -0.24952314,  0.19402091, -0.17759545, -0.35022736,\n",
       "         0.10147858],\n",
       "       [-0.23858436,  0.27493048, -0.44996253,  0.07895978,  0.21873452,\n",
       "        -0.0023886 ,  0.36592473, -0.1291103 , -0.07877531, -0.5337351 ,\n",
       "         0.41144893],\n",
       "       [ 0.46363166, -0.15179136,  0.23824707, -0.07941826, -0.05857268,\n",
       "         0.0709103 ,  0.62167708, -0.38144967, -0.37751558,  0.10549701,\n",
       "         0.06959338],\n",
       "       [ 0.14610715,  0.27208024,  0.10128338, -0.37279256,  0.73214429,\n",
       "        -0.18402996,  0.09287208,  0.00752295,  0.29984469,  0.29066341,\n",
       "         0.04915555],\n",
       "       [ 0.21224658,  0.14805156, -0.09261383,  0.66619476,  0.2465009 ,\n",
       "        -0.05306532, -0.21767112,  0.11133867, -0.35700936,  0.37041337,\n",
       "         0.30433857],\n",
       "       [-0.03615752,  0.51356681,  0.42879287, -0.04353782, -0.15915198,\n",
       "         0.05142086,  0.24848326,  0.63540522, -0.2047805 , -0.11659611,\n",
       "        -0.01400021],\n",
       "       [ 0.02357485,  0.56948696,  0.3224145 , -0.03457712, -0.22246456,\n",
       "        -0.0687016 , -0.37075027, -0.59211589,  0.01903597, -0.09366237,\n",
       "         0.13630755],\n",
       "       [ 0.39535301,  0.23357549, -0.33887135, -0.17449976,  0.15707671,\n",
       "         0.5673319 , -0.23999012,  0.02071868, -0.23922267, -0.17048116,\n",
       "        -0.3911523 ],\n",
       "       [-0.43851962,  0.00671079,  0.05769735, -0.00378775,  0.26752977,\n",
       "        -0.3407109 , -0.0109696 , -0.16774589, -0.56139075, -0.02513762,\n",
       "        -0.52211645],\n",
       "       [ 0.24292133, -0.03755392,  0.27978615,  0.55087236,  0.22596222,\n",
       "        -0.06955538,  0.11232046, -0.05836706,  0.37460432, -0.44746911,\n",
       "        -0.38126343],\n",
       "       [-0.11323207, -0.38618096,  0.47167322, -0.12218109,  0.35068141,\n",
       "         0.31452591, -0.3030145 ,  0.03760311, -0.21762556, -0.3276509 ,\n",
       "         0.36164504]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.1010718226758947, array([ 0.48931422, -0.23858436,  0.46363166,  0.14610715,  0.21224658,\n",
      "       -0.03615752,  0.02357485,  0.39535301, -0.43851962,  0.24292133,\n",
      "       -0.11323207]))\n",
      "(1.9271148896490469, array([-0.11050274,  0.27493048, -0.15179136,  0.27208024,  0.14805156,\n",
      "        0.51356681,  0.56948696,  0.23357549,  0.00671079, -0.03755392,\n",
      "       -0.38618096]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Choose the best n principal components.  Calculate newly extracted feature data.\"\"\"\n",
    "\n",
    "eig_pairs = [(np.abs(eigenValues[i]), eigenVectors[:,i]) for i in range(len(eigenValues))]\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "for i in eig_pairs[:2]:\n",
    "    print(i[0],i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The information (explained variance) contained in each principal component: ', array([ 0.28173931,  0.1750827 ]))\n",
      "[[ 0.48931422 -0.23858436  0.46363166  0.14610715  0.21224658 -0.03615752\n",
      "   0.02357485  0.39535301 -0.43851962  0.24292133 -0.11323207]\n",
      " [-0.11050274  0.27493048 -0.15179136  0.27208024  0.14805156  0.51356681\n",
      "   0.56948696  0.23357549  0.00671079 -0.03755392 -0.38618096]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Compare with sklearn's PCA method.\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(x_standard)\n",
    "print(\"The information (explained variance) contained in each principal component: \", pca.explained_variance_ratio_)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now what?\n",
    "\n",
    "We can use this to transform our data onto a lower dimension space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.61952988,  0.45095009],\n",
       "       [-0.79916993,  1.85655306],\n",
       "       [-0.74847909,  0.88203886],\n",
       "       ..., \n",
       "       [-1.45612897,  0.31174559],\n",
       "       [-2.27051793,  0.97979111],\n",
       "       [-0.42697475, -0.53669021]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.hstack((eig_pairs[0][1].reshape(11,1), eig_pairs[1][1].reshape(11,1))) # Our transformation matrix\n",
    "X_reduced = x_standard.dot(W)\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48931422],\n",
       "       [-0.23858436],\n",
       "       [ 0.46363166],\n",
       "       [ 0.14610715],\n",
       "       [ 0.21224658],\n",
       "       [-0.03615752],\n",
       "       [ 0.02357485],\n",
       "       [ 0.39535301],\n",
       "       [-0.43851962],\n",
       "       [ 0.24292133],\n",
       "       [-0.11323207]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_pairs[0][1].reshape(11,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11050274],\n",
       "       [ 0.27493048],\n",
       "       [-0.15179136],\n",
       "       [ 0.27208024],\n",
       "       [ 0.14805156],\n",
       "       [ 0.51356681],\n",
       "       [ 0.56948696],\n",
       "       [ 0.23357549],\n",
       "       [ 0.00671079],\n",
       "       [-0.03755392],\n",
       "       [-0.38618096]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_pairs[1][1].reshape(11,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48931422, -0.11050274],\n",
       "       [-0.23858436,  0.27493048],\n",
       "       [ 0.46363166, -0.15179136],\n",
       "       [ 0.14610715,  0.27208024],\n",
       "       [ 0.21224658,  0.14805156],\n",
       "       [-0.03615752,  0.51356681],\n",
       "       [ 0.02357485,  0.56948696],\n",
       "       [ 0.39535301,  0.23357549],\n",
       "       [-0.43851962,  0.00671079],\n",
       "       [ 0.24292133, -0.03755392],\n",
       "       [-0.11323207, -0.38618096]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joce/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630681818182 mean accuracy, using 11 dimensions.\n",
      "0.49053030303 mean accuracy, using 2 principal component dimensions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_standard, y, test_size=0.33, random_state=1)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print clf.score(X_test, y_test), \"mean accuracy, using {0} dimensions.\".format(x_standard.shape[1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.33, random_state=1)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print clf.score(X_test, y_test), \"mean accuracy, using {0} principal component dimensions.\".format(X_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice: Dimensionality Reduction on the Iris dataset (20 minutes)\n",
    "\n",
    "Now that we've gone over the long-form approach to dimensionality reduction and worked through an example, let's put your skills to the test! We're going to be working with the classic [iris dataset](./assets/datasets/iris.csv). We want to decompose the data to the point of finding the eigenvectors and eigenvalues. Grab the [starter code](./code/starter-code/w7d2-dimensionality-reduction-iris-starter-code.ipynb) to begin!\n",
    "\n",
    "> Note: [solution code](./code/solution-code/w7d2-dimensionality-reduction-iris-solution-code.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 mins)\n",
    "- Recap and recall the process steps in dimensionality reduction\n",
    "    -  Covariance Matrix: First, we create a covariance matrix to decompose so that we may find our eigenvalues / eigenvectors. \n",
    "    -  Eigenvectors & Eigenvalues: We decompose the covariance matrix to derive our eigenvectors and eigenvalues, and select the top  combined eigenpairs to become our principal components.\n",
    "    -  Lastly, we project the eigenpairs onto a new feature subspace.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Unsupervised Dimensionality Reduction in sklearn](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)\n",
    "- [In depth overview of Dimensionality Reduction and PCA from Stanford University](http://ufldl.stanford.edu/wiki/index.php/PCA)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
